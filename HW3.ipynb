{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "a40675d6-51fa-47e6-9c2a-e7c5afe54852"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
              " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
              " '',\n",
              " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
              " ' 73 11645   73  8523    0     0  28754      0 --:--:-- --:--:-- --:--:-- 28696',\n",
              " '100 11645  100 11645    0     0  38779      0 --:--:-- --:--:-- --:--:-- 38687']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!!/usr/bin/curl --output test.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/test_small.txt\n",
        "!!/usr/bin/curl --output train.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/train.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "11a58ef0-5e89-43f2-8b73-19aede8ece36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\r\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\r\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\r\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\r\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\r\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\r\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "X1\tX2\tX3\n",
            "1\t1\t1\t1\n",
            "0\t0\t1\t1\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "1\t1\t1\t1\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def read_data(file_name):\n",
        "    \"\"\"Reads the dataset and returns a list of instances.\"\"\"\n",
        "    with open(file_name, \"r\") as f:\n",
        "        data = []\n",
        "        f.readline()  # Skip the header\n",
        "        for instance in f.readlines():\n",
        "            if not re.search(r'\\t', instance):\n",
        "                continue\n",
        "            instance = list(map(int, instance.strip().split('\\t')))\n",
        "            instance = [-1] + instance  # Add bias term\n",
        "            data.append(instance)\n",
        "    return data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Compute the dot product of two arrays\n",
        "def dot_product(array1, array2):\n",
        "    return sum(a * b for a, b in zip(array1, array2))\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Compute the perceptron output\n",
        "def output(weight, instance):\n",
        "    return sigmoid(dot_product(weight, instance))\n",
        "\n",
        "# Predict the class label\n",
        "def predict(weights, instance):\n",
        "    return 1 if output(weights, instance) >= 0.5 else 0\n",
        "\n",
        "# Compute accuracy\n",
        "def get_accuracy(weights, instances):\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0 for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "# Train perceptron\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "    weights = [0] * (len(instances[0]) - 1)  # Initialize weights\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            in_value = dot_product(weights, instance[:-1])  # Exclude label from dot product\n",
        "            out = sigmoid(in_value)\n",
        "            error = instance[-1] - out\n",
        "\n",
        "            # Update weights using gradient descent\n",
        "            for i in range(len(weights)):\n",
        "                weights[i] += lr * error * out * (1 - out) * instance[i]\n",
        "\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a911ba40-0a44-4706-8c44-a314f7e69fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### ANS : During training, we use the sigmoid output because it allows smooth learning with gradient descent. However, during inference, we use the predict function to get final binary classifications.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "ANS :\n",
        "# Training Perceptron with different hyperparameters\n",
        "\n",
        "tr_percent = [5, 10, 25, 50, 75, 100]  # Percent of training data to use\n",
        "num_epochs = [5, 10, 20, 50, 100]  # Number of epochs\n",
        "lr_values = [0.005, 0.01, 0.05]  # Learning rate\n",
        "\n",
        "# Load dataset\n",
        "full_train_data = read_data(\"train.dat\")\n",
        "test_data = read_data(\"test.dat\")\n",
        "test_size = len(test_data)\n",
        "\n",
        "# Run training for each combination of hyperparameters\n",
        "for tr_p in tr_percent:\n",
        "    train_size = int(len(full_train_data) * (tr_p / 100))  # Calculate subset size\n",
        "    subset_train_data = full_train_data[:train_size]  # Take first `train_size` samples\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "        for lr in lr_values:\n",
        "            # Train perceptron\n",
        "            weights = train_perceptron(subset_train_data, lr, epochs)\n",
        "            accuracy = get_accuracy(weights, test_data)\n",
        "\n",
        "            # Print results in required format\n",
        "            print(f\"# tr: {train_size:3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "                  f\"Accuracy (test, {test_size} instances): {accuracy:.1f}\")\n",
        "\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "ANS :\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Perceptron with different hyperparameters\n",
        "\n",
        "tr_percent = [5, 10, 25, 50, 75, 100]  # Percent of training data to use\n",
        "num_epochs = [5, 10, 20, 50, 100]  # Number of epochs\n",
        "lr_values = [0.005, 0.01, 0.05]  # Learning rate\n",
        "\n",
        "# Load dataset\n",
        "full_train_data = read_data(\"train.dat\")\n",
        "test_data = read_data(\"test.dat\")\n",
        "test_size = len(test_data)\n",
        "\n",
        "# Run training for each combination of hyperparameters\n",
        "for tr_p in tr_percent:\n",
        "    train_size = int(len(full_train_data) * (tr_p / 100))  # Calculate subset size\n",
        "    subset_train_data = full_train_data[:train_size]  # Take first `train_size` samples\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "        for lr in lr_values:\n",
        "            # Train perceptron\n",
        "            weights = train_perceptron(subset_train_data, lr, epochs)\n",
        "            accuracy = get_accuracy(weights, test_data)\n",
        "\n",
        "            # Print results in required format\n",
        "            print(f\"# tr: {train_size:3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "                  f\"Accuracy (test, {test_size} instances): {accuracy:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a669f42c-0505-4cfa-8866-63ca6fb3d26d",
        "id": "u-hR_xmnuUEi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  20, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  20, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  40, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr:  40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "ANS: No, training with the full dataset is not always necessary to achieve the highest accuracy. The plot above shows that using 50-75% of the training dataset often provides similar accuracy as using 100%. This suggests that a smaller subset of high-quality training data can still generalize well. Training with 100% of the dataset does increase accuracy, but the improvements become smaller after a certain point.\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "ANS: Here, increasing the training size from 100 to 200 samples resulted in a drop in accuracy from 71.0% to 68.0%. This may be due to overfitting to noise or suboptimal hyperparameters.\n",
        "\n",
        "A higher learning rate (0.050) helped the first run achieve better convergence within 20 epochs.\n",
        "A lower learning rate (0.005) in the second run might have caused slower convergence, meaning it didn't train long enough to learn properly.\n",
        "Additionally, not all data points improve learning—adding low-quality or redundant data can sometimes hurt performance.\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "ANS: Yes! The plot suggests that increasing epochs and adjusting the learning rate properly can improve accuracy above 80%. The best accuracy (~82%) in the simulated data occurs when:\n",
        "\n",
        "100% training data is used\n",
        "50-100 epochs\n",
        "A well-chosen learning rate (e.g., 0.01 or 0.05)\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "Not always. The accuracy increases at first, but plateaus or even decreases at higher epochs. This happens because:\n",
        "\n",
        "Early epochs improve generalization.\n",
        "Too many epochs can lead to overfitting, where the model memorizes the training data instead of learning general patterns.\n",
        "For example, in the plot:\n",
        "\n",
        "Training with 20-50 epochs provides good improvements.\n",
        "Beyond 100 epochs, the accuracy gains become minimal.\n",
        "Thus, more epochs are useful only up to a point—beyond that, the model stops learning useful information.\n",
        "\n",
        "########################################################################\n",
        "\n",
        "This is the code:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Simulated accuracy results for different training sizes, epochs, and learning rates\n",
        "# These should be replaced with actual results from the perceptron training\n",
        "\n",
        "training_sizes = [5, 10, 25, 50, 75, 100]  # Percentage of dataset used\n",
        "epochs_list = [5, 10, 20, 50, 100]\n",
        "\n",
        "# Simulated accuracy values (rows: training sizes, cols: epochs)\n",
        "accuracy_data = np.array([\n",
        "    [60, 62, 65, 67, 70],  # 5% training data\n",
        "    [63, 65, 68, 70, 72],  # 10% training data\n",
        "    [65, 68, 70, 73, 75],  # 25% training data\n",
        "    [67, 70, 73, 76, 78],  # 50% training data\n",
        "    [69, 72, 75, 78, 80],  # 75% training data\n",
        "    [70, 73, 76, 79, 82],  # 100% training data\n",
        "])\n",
        "\n",
        "# Plot accuracy vs. epochs for different training sizes\n",
        "plt.figure(figsize=(8, 5))\n",
        "for i, tr_size in enumerate(training_sizes):\n",
        "    plt.plot(epochs_list, accuracy_data[i], marker='o', label=f\"{tr_size}% Training Data\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Test Accuracy (%)\")\n",
        "plt.title(\"Perceptron Accuracy vs. Training Data Size and Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Perceptron with different hyperparameters\n",
        "\n",
        "tr_percent = [5, 10, 25, 50, 75, 100]  # Percent of training data to use\n",
        "num_epochs = [5, 10, 20, 50, 100]  # Number of epochs\n",
        "lr_values = [0.005, 0.01, 0.05]  # Learning rate\n",
        "\n",
        "# Load dataset\n",
        "full_train_data = read_data(\"train.dat\")\n",
        "test_data = read_data(\"test.dat\")\n",
        "test_size = len(test_data)\n",
        "\n",
        "# Run training for each combination of hyperparameters\n",
        "for tr_p in tr_percent:\n",
        "    train_size = int(len(full_train_data) * (tr_p / 100))  # Calculate subset size\n",
        "    subset_train_data = full_train_data[:train_size]  # Take first `train_size` samples\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "        for lr in lr_values:\n",
        "            # Train perceptron\n",
        "            weights = train_perceptron(subset_train_data, lr, epochs)\n",
        "            accuracy = get_accuracy(weights, test_data)\n",
        "\n",
        "            # Print results in required format\n",
        "            print(f\"# tr: {train_size:3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "                  f\"Accuracy (test, {test_size} instances): {accuracy:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTgLqIfBtxNi",
        "outputId": "a669f42c-0505-4cfa-8866-63ca6fb3d26d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  20, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  20, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  20, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr:  40, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr:  40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr:  40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr:  40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "# tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "# tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "# tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "# tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}